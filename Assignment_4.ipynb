{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNB8aDIZIZyQt7QAw0RXolA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VrushaliLad799/707-lecture-master/blob/main/Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 4**\n",
        "\n",
        "Did you follow the rules? If not, your assignment will be returned ungraded.\n",
        "\n",
        "Notebook organization:\n",
        "Two major sections, one for each dataset. Each section contains:\n",
        "Data generation code (5 points)\n",
        "Data preprocessing as necessary, including scaling, encoding (5 points)\n",
        "Proper model execution with 5-fold cross validation on each model (10 points)\n",
        "\n",
        "Did you achieve at least a .02 difference in performance on both datasets?"
      ],
      "metadata": {
        "id": "cGpPXjR_wc6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# M1 = LogisticRegression,\n",
        "# M2 = DecisionTreeClassifier\n",
        "\n",
        "m1 = LogisticRegression()\n",
        "m2 = DecisionTreeClassifier()\n",
        "\n",
        "# Dataset 1 - Favors Logistic Regression\n",
        "X1, y1 = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=3, n_clusters_per_class=1, random_state=10)\n",
        "\n",
        "# Scale dataset 1\n",
        "scaler = StandardScaler()\n",
        "X1 = scaler.fit_transform(X1)\n",
        "\n",
        "#Cross val score with f1 as scoring\n",
        "m1_scores_A = cross_val_score(m1, X1, y1, cv=5, scoring='f1')\n",
        "m2_scores_A = cross_val_score(m2, X1, y1, cv=5, scoring='f1')\n",
        "\n",
        "print(\"Dataset 1:\")\n",
        "print(\"Logistic Regression F1: {:.2f}\".format(m1_scores_A.mean()))\n",
        "print(\"Decision Tree Classifier F1: {:.2f}\".format(m2_scores_A.mean()))"
      ],
      "metadata": {
        "id": "sqrnQIgQDZ_3",
        "outputId": "b93e5038-7561-4127-e880-d0dedd0a0ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 1:\n",
            "Logistic Regression F1: 0.93\n",
            "Decision Tree Classifier F1: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 2 - Decision Tree Classifier\n",
        "X2, y2 = make_classification(n_samples=10000, n_features=100, n_informative=4, n_redundant=50, n_clusters_per_class=4, random_state=10)\n",
        "\n",
        "\n",
        "# Scale dataset 2\n",
        "scaler = StandardScaler()\n",
        "X2 = scaler.fit_transform(X2)\n",
        "\n",
        "#Cross val score with f1 as scoring\n",
        "m1_scores_B = cross_val_score(m1, X2, y2, cv=5, scoring='f1')\n",
        "m2_scores_B = cross_val_score(m2, X2, y2, cv=5, scoring='f1')\n",
        "\n",
        "print(\"\\nDataset 2:\")\n",
        "print(\"Logistic Regression F1: {:.2f}\".format(m1_scores_B.mean()))\n",
        "print(\"Decision Tree Classifier F1: {:.2f}\".format(m2_scores_B.mean()))"
      ],
      "metadata": {
        "id": "hiVhfzthEDFP",
        "outputId": "beefd525-f608-4c81-b98c-032889efaa47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset 2:\n",
            "Logistic Regression F1: 0.67\n",
            "Decision Tree Classifier F1: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving datasets A and B as csv files\n",
        "import pandas as pd\n",
        "\n",
        "# For dataset A\n",
        "data_A = pd.DataFrame(X1)\n",
        "data_A['target'] = y1\n",
        "data_A.to_csv('data_A.csv', index=False)\n",
        "\n",
        "# For dataset B\n",
        "data_B = pd.DataFrame(X2)\n",
        "data_B['target'] = y2\n",
        "data_B.to_csv('data_B.csv', index=False)"
      ],
      "metadata": {
        "id": "dMQhaEg9Eht8"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Dataset 1:***\n",
        "\n",
        "Logistic Regression F1: **0.93**\n",
        "\n",
        "Decision Tree Classifier F1: **0.87**\n",
        "\n",
        "Difference: **0.06**\n",
        "\n",
        "***Dataset 2:***\n",
        "\n",
        "Logistic Regression F1: **0.67**\n",
        "\n",
        "Decision Tree Classifier F1: **0.87**\n",
        "\n",
        "Difference: **0.20**\n",
        "\n",
        "From above we can see Logistic Regression performs best on Dataset 1 whereas Decision Tree Classifier performs poor.\n",
        "\n",
        "The decision Tree Classifier performs best on Dataset 2 whereas Logistic Regression performs poor.\n",
        "\n",
        "The minimal discrepancy is in Dataset 1 which is **0.06**"
      ],
      "metadata": {
        "id": "yvrzgptxG60M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Findings:***\n",
        "\n",
        "\n",
        "Dataset 1 favors Logistic Regression (M1) because it has 10 informative features and only 3 redundant ones. Logistic Regression works best with a moderate number of informative features and fewer irrelevant ones.\n",
        "\n",
        "Dataset 2 favors Decision Tree (M2) because it has only 4 informative features out of 100 total features, and a significant number (50) of redundant features. Decision Trees can handle large numbers of irrelevant features and complex interactions.\n",
        "\n",
        "Logistic Regression is suitable for\n",
        "small datasets with linearly separable features,\n",
        "with a moderate number of informative features\n",
        "low redundancy.\n",
        "\n",
        "Decision Trees are effective for large datasets with non-linear decision boundaries and a mix of relevant and irrelevant features.when there are few informative features among many irrelevant ones, or when there are complex interactions between features.\n",
        "\n"
      ],
      "metadata": {
        "id": "rclQmnCdG7t3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SnDETmTcHB1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}